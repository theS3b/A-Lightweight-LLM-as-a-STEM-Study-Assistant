{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b48640-cb04-45cb-8330-5ea570eefd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, json, torch, logging, os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor import configure_logger, LoggerConfig\n",
    "\n",
    "from mmlu_eval import evaluate_mmlu, mmlu_harness_hf, display_metric\n",
    "\n",
    "configure_logger(LoggerConfig(\n",
    "    disabled=True,\n",
    "    clear_loggers=True,\n",
    "    console_log_level=None,\n",
    "    log_file=None,\n",
    "    log_file_level=None\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5feb48ee-bbd8-4ade-94a2-9314a411d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_prefix = \"TheS3b/Qwen3-0.6B-SmoothQuant-W8A8-calib\"  # base name for HF pushes\n",
    "model_repo = \"brygotti/MNLP_M2_mcqa_model\"\n",
    "BITS         = 8\n",
    "BLOCK_SIZE   = 64\n",
    "prompt_sizes = [20, 200, 2000]\n",
    "MAX_SEQUENCE_LENGTH = 2048\n",
    "\n",
    "logging.disable(logging.INFO)\n",
    "os.environ[\"EXLLAMA_KERNELS_AVAILABLE\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc39c42-b6cf-4932-a28e-ad4bd5bd54bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── SmoothQuant W8A8 with 20 calibration prompts ──\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 24.41it/s]\n",
      "Preparing intermediates cache: 100%|██████████| 20/20 [00:00<00:00, 2246.73it/s]\n",
      "(1/29): Calibrating: 100%|██████████| 20/20 [00:00<00:00, 293.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer once\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo, trust_remote_code=True)\n",
    "\n",
    "# Re-use the calibration set you already filtered\n",
    "calibration_data = load_dataset(\"TheS3b/unified-dataset-filtered-430K\")\n",
    "def is_valid_prompt(example, min_len=64, max_len=256, thresh=0.5):\n",
    "    tokens = tokenizer(example[\"prompt\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return min_len <= tokens.shape[1] <= max_len and (example[\"relevance1\"] + example[\"relevance2\"]) * 0.5 > thresh\n",
    "filtered_calibration_set = calibration_data.filter(\n",
    "    lambda ex: is_valid_prompt(ex), batched=False\n",
    ").shuffle(seed=42)[\"train\"]\n",
    "\n",
    "# Tokenisation helper for llm-compressor (expects tokenised samples)\n",
    "def tokenise(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"prompt\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "# Your evaluation set and helpers (unchanged)\n",
    "eval_ds = load_dataset(\"brygotti/NLP4Education_english_single_mcq_4_choices\")[\"test\"]\n",
    "\n",
    "for size in prompt_sizes:\n",
    "    print(f\"\\n── SmoothQuant W{BITS}A8 with {size} calibration prompts ──\\n\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Build tokenised calibration dataset\n",
    "    calib_ds = filtered_calibration_set.select(range(size)).map(\n",
    "        tokenise, remove_columns=filtered_calibration_set.column_names\n",
    "    )\n",
    "\n",
    "    # FP16 baseline model (no quantisation yet)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_repo,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # Compression “recipe”: SmoothQuant first, *optional* GPTQ second\n",
    "    recipe = [\n",
    "        SmoothQuantModifier(\n",
    "            smoothing_strength=0.8,\n",
    "            ignore=[\"lm_head\"],\n",
    "            num_calibration_steps=size,\n",
    "            block_size=BLOCK_SIZE,\n",
    "        ),\n",
    "        GPTQModifier(\n",
    "            scheme=f\"W{BITS}A8\",\n",
    "            targets=\"Linear\",\n",
    "            ignore=[\"lm_head\"],\n",
    "            block_size=BLOCK_SIZE,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # One-shot quantisation pass\n",
    "    oneshot(\n",
    "        model=model,\n",
    "        dataset=calib_ds,\n",
    "        recipe=recipe,\n",
    "        max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "        num_calibration_samples=size,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    metrics = evaluate_mmlu(eval_ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "    display_metric(f\"SmoothQuant W{BITS}A8 Size {size}\", metrics)\n",
    "    key = f\"SmoothQuant W{BITS}A8 calib{size}\"\n",
    "    all_metrics[key] = metrics\n",
    "\n",
    "    push_name = f\"{hub_prefix}{size}\"\n",
    "    tokenizer.push_to_hub(push_name)\n",
    "    model.push_to_hub(push_name)\n",
    "\n",
    "    with open(\"Results/smooth_quant_metrics_calibration.json\", \"w\") as f:\n",
    "        json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb0cea-c2dc-420f-aed9-898fd605c3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc2536-715d-48b6-bd4d-0b68bf1c5540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp_compute",
   "language": "python",
   "name": "mnlp_compute"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
