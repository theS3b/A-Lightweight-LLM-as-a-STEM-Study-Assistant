{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22ba715-d2fd-42a5-ba4d-59c8545ed504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, Value\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPTQConfig\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import json\n",
    "from evaluation_utils import evaluate_mmlu, mmlu_harness_hf, display_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8df575-b73b-4fb1-938a-614cce60f7f3",
   "metadata": {},
   "source": [
    "# EfficientQAT 4-bit group size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbc3dde-1913-4745-ab55-9322098726ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2a43ed-c9a5-405a-bf7a-2e3c2ab61a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaV2QuantLinear`         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Evaluating: 100%|██████████| 1962/1962 [04:50<00:00,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**QAT-W4G64 Evaluation Results**\n",
      "- Accuracy              :  45.87 %\n",
      "- Avg. inference time   :  147.0 ms\n",
      "- Throughput (tok/s)    : 2838.9\n",
      "- Avg. peak VRAM        :  792.1 MB\n",
      "- Score Acc/VRAM        :  0.579 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "repo_id = 'TheS3b/Qwen3-EfficientQAT-w4g64'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ds = load_dataset(\"brygotti/NLP4Education_english_single_mcq_4_choices\")[\"test\"]\n",
    "metrics = evaluate_mmlu(ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "display_metric(\"QAT-W4G64\", metrics)\n",
    "all_metrics['QAT-W4G64'] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c573ff-a5f0-4600-afa9-9217962222fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f51e718f-36fe-4160-949f-bc66cd80cc50",
   "metadata": {},
   "source": [
    "# EfficientQAT 2-bit group size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e6f6aa-966a-4ab8-b452-60ba0d5d8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1962/1962 [07:46<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**QAT-W2G64 Evaluation Results**\n",
      "- Accuracy              :  37.26 %\n",
      "- Avg. inference time   :  236.6 ms\n",
      "- Throughput (tok/s)    : 1763.8\n",
      "- Avg. peak VRAM        :  486.4 MB\n",
      "- Score Acc/VRAM        :  0.766 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "repo_id = 'TheS3b/Qwen3-EfficientQAT-w2g64'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ds = load_dataset(\"brygotti/NLP4Education_english_single_mcq_4_choices\")[\"test\"]\n",
    "metrics = evaluate_mmlu(ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "display_metric(\"QAT-W2G64\", metrics)\n",
    "all_metrics['QAT-W2G64'] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a91e205-6cbf-4d36-95b7-aceb9af72238",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Results/QAT-metrics.json\", \"w\") as f:\n",
    "    json.dump(all_metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b41cec-5e16-4247-88ba-527652c3a564",
   "metadata": {},
   "source": [
    "# MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4c00b3-e0a8-49ee-bfe8-58374dcf08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4c8607-f559-490b-96cb-01825f8263fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaV2QuantLinear`         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Evaluating: 100%|██████████| 14042/14042 [35:39<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**QAT-W4G64-MMLU Evaluation Results**\n",
      "- Accuracy              :  49.77 %\n",
      "- Avg. inference time   :  151.3 ms\n",
      "- Throughput (tok/s)    : 3184.7\n",
      "- Avg. peak VRAM        :  922.4 MB\n",
      "- Score Acc/VRAM        :  0.540 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "repo_id = 'TheS3b/Qwen3-EfficientQAT-w4g64'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ds = load_dataset(\"brygotti/mmlu\")[\"test\"]\n",
    "metrics = evaluate_mmlu(ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "display_metric(\"QAT-W4G64-MMLU\", metrics)\n",
    "all_metrics['QAT-W4G64-MMLU'] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91b0b93-7578-4bfe-9baf-c98aa7e546ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Evaluating: 100%|██████████| 14042/14042 [56:25<00:00,  4.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**QAT-W2G64-MMLU Evaluation Results**\n",
      "- Accuracy              :  37.72 %\n",
      "- Avg. inference time   :  240.1 ms\n",
      "- Throughput (tok/s)    : 2007.7\n",
      "- Avg. peak VRAM        :  495.8 MB\n",
      "- Score Acc/VRAM        :  0.761 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "repo_id = 'TheS3b/Qwen3-EfficientQAT-w2g64'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ds = load_dataset(\"brygotti/mmlu\")[\"test\"]\n",
    "metrics = evaluate_mmlu(ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "display_metric(\"QAT-W2G64-MMLU\", metrics)\n",
    "all_metrics['QAT-W2G64-MMLU'] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a50968-996b-45ac-9dc4-c2f6ec5d3276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15af85096b864cc293b6fcff2c62a82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6421de723e43aba33afe608c6958c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b7b4d5fb0b4282bdf9a0fc45a4b65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24e6bc5348c4bcda1ec22be7d1cb2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c7db4364694e6dbad0a992e6f2baf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b084f6f39854a0cbb1b37175d5c9b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5add8fd0e348ecbd46befe4d05c6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/4.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8cb953090e45ab90a280bd63dafe70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `MarlinQuantLinear`            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018ceacff9bd4634953a57e920f7035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 14042/14042 [35:13<00:00,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**GPTQ-4bit-MMLU Evaluation Results**\n",
      "- Accuracy              :  48.02 %\n",
      "- Avg. inference time   :  149.5 ms\n",
      "- Throughput (tok/s)    : 3224.6\n",
      "- Avg. peak VRAM        :  714.3 MB\n",
      "- Score Acc/VRAM        :  0.672 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "repo_id = 'TheS3b/Qwen3-0.6B-GPTQ-4bit-rel0.5-calib200'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ds = load_dataset(\"brygotti/mmlu\")[\"test\"]\n",
    "metrics = evaluate_mmlu(ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "display_metric(\"GPTQ-4bit-MMLU\", metrics)\n",
    "all_metrics['GPTQ-4bit-MMLU'] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b2de476-575d-468e-bac3-e71e95c02281",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Results/QAT-metrics-MMLU.json\", \"w\") as f:\n",
    "    json.dump(all_metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0095e4-02ea-46ed-b0dc-2f9d7d594392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "mnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
