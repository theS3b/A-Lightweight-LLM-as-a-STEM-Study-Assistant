{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acc9a51-da07-4354-a1b8-139b44c5874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, Value\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPTQConfig\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31109d15-c44f-47d0-b35c-053429306e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "LETTER_INDICES: List[str] = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "\n",
    "class Doc:\n",
    "    def __init__(self, query: str, choices: List[str], gold_index: int):\n",
    "        self.query = query\n",
    "        self.choices = choices\n",
    "        self.gold_index = gold_index\n",
    "\n",
    "def mmlu_harness_hf(\n",
    "    ex, topic: str = \"advanced master-level STEM courses\"\n",
    ") -> Doc:\n",
    "    \"\"\"\n",
    "    Convert a raw example from the HF MMLU JSON into a Doc object\n",
    "    understood by the evaluator.\n",
    "    \"\"\"\n",
    "    question = ex[\"question\"]\n",
    "    choices = ex[\"choices\"]\n",
    "    answer = ex[\"answer\"]\n",
    "    prompt = f\"The following are multiple choice questions about {topic}.\\n\\n\"\n",
    "    prompt += question + \"\\n\"\n",
    "    for letter, text in zip(LETTER_INDICES, choices):\n",
    "        prompt += f\"{letter}. {text}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    gold_ix = LETTER_INDICES.index(answer)\n",
    "    # prepend a space before each candidate, as required by the original prompt logic\n",
    "    return Doc(prompt, [f\" {c}\" for c in LETTER_INDICES], gold_ix)\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_choice(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    device: torch.device,\n",
    "    prompt: str,\n",
    "    choice: str,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Log-probability that `model` assigns to `choice` when it is generated\n",
    "    directly after `prompt`.\n",
    "    \"\"\"\n",
    "    # 1) Encode prompt and prompt+choice (no special tokens)\n",
    "    enc_prompt = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", add_special_tokens=False\n",
    "    ).to(device)\n",
    "    enc_full = tokenizer(\n",
    "        prompt + choice, return_tensors=\"pt\", add_special_tokens=False\n",
    "    ).to(device)\n",
    "\n",
    "    input_ids = enc_full.input_ids\n",
    "    attn_mask = enc_full.attention_mask\n",
    "\n",
    "    # 2) Forward pass\n",
    "    logits = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # 3) Sum log-probs only for the choice tokens\n",
    "    prompt_len = enc_prompt.input_ids.size(1)\n",
    "    total_lp = 0.0\n",
    "    for i in range(prompt_len, input_ids.size(1)):\n",
    "        token_id = input_ids[0, i].item()\n",
    "        total_lp += log_probs[0, i - 1, token_id].item()\n",
    "\n",
    "    return total_lp\n",
    "\n",
    "def evaluate_mmlu(\n",
    "    dataset,\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    device: torch.device,\n",
    "    harness_fn,\n",
    ") -> Dict[str, float]:\n",
    "\n",
    "    correct = total = 0\n",
    "    total_time = 0.0\n",
    "    total_tokens = 0\n",
    "    per_example_peaks: List[int] = []\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Evaluating\"):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        doc = harness_fn(example)\n",
    "\n",
    "        for choice in doc.choices:\n",
    "            # same construction we pass to the model\n",
    "            ids = tokenizer(doc.query + choice,\n",
    "                            add_special_tokens=False).input_ids\n",
    "            total_tokens += len(ids)\n",
    "\n",
    "        scores = [\n",
    "            score_choice(model, tokenizer, device, doc.query, c)\n",
    "            for c in doc.choices\n",
    "        ]\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize(device)\n",
    "\n",
    "        total_time += time.perf_counter() - start\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            per_example_peaks.append(torch.cuda.max_memory_allocated(device))\n",
    "\n",
    "        pred = int(torch.argmax(torch.tensor(scores)))\n",
    "        correct += (pred == doc.gold_index)\n",
    "        total += 1\n",
    "\n",
    "    avg_time      = total_time / total\n",
    "    tokens_per_s  = total_tokens / total_time\n",
    "    avg_peak_vram = (\n",
    "        (sum(per_example_peaks) / len(per_example_peaks)) / 1024**2\n",
    "        if per_example_peaks else float(\"nan\")\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": correct / total,\n",
    "        \"avg_time_s\": avg_time,\n",
    "        \"tokens_per_s\": tokens_per_s,\n",
    "        \"avg_peak_vram_MB\": avg_peak_vram,\n",
    "        \"score_acc_over_vram\": 1000 * (correct / total) / avg_peak_vram\n",
    "    }\n",
    "\n",
    "\n",
    "def display_metric(name, metrics):\n",
    "    print(\n",
    "        f\"\\n**{name} Evaluation Results**\\n\"\n",
    "        f\"- Accuracy              : {metrics['accuracy'] * 100:6.2f} %\\n\"\n",
    "        f\"- Avg. inference time   : {metrics['avg_time_s'] * 1_000:6.1f} ms\\n\"\n",
    "        f\"- Throughput (tok/s)    : {metrics['tokens_per_s']:6.1f}\\n\"\n",
    "        f\"- Avg. peak VRAM        : {metrics['avg_peak_vram_MB']:6.1f} MB\\n\"\n",
    "        f\"- Score Acc/VRAM        : {metrics['score_acc_over_vram']:6.3f} \\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c75061-7ab0-4edc-8aec-4de03197eeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'choices', 'answer'],\n",
      "        num_rows: 1962\n",
      "    })\n",
      "})\n",
      "['id', 'question', 'choices', 'answer']\n",
      "1962\n"
     ]
    }
   ],
   "source": [
    "# 1) load the HF dataset\n",
    "ds = load_dataset(\"brygotti/NLP4Education_english_single_mcq_4_choices\")\n",
    "print(ds)\n",
    "print(ds[\"test\"].column_names)\n",
    "print(len(ds['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e18589-0dfe-4fd7-bca3-fe5eda291562",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_repo  = \"Qwen/Qwen3-0.6B-Base\"\n",
    "model_repo = \"brygotti/MNLP_M2_mcqa_model\"\n",
    "\n",
    "all_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b62a73-8c58-4f5c-b28a-d6107321bb80",
   "metadata": {},
   "source": [
    "# Simple Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4955f35e-5c9d-46a8-99fc-dedca092c1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1962/1962 [05:06<00:00,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Base Model Evaluation Results**\n",
      "- Accuracy              :  44.34 %\n",
      "- Avg. inference time   :  155.1 ms\n",
      "- Throughput (tok/s)    : 2690.3\n",
      "- Avg. peak VRAM        : 2402.8 MB\n",
      "- Score Acc/VRAM        :  0.185 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_repo, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_repo,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ds = load_dataset(\"brygotti/NLP4Education_english_single_mcq_4_choices\")[\"test\"]\n",
    "metrics = evaluate_mmlu(ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "display_metric(\"Base Model\", metrics)\n",
    "all_metrics['Base Model'] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba91ca0-3cb0-48ed-bbc1-33eee89e008d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1962/1962 [05:11<00:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**SFT Model Evaluation Results**\n",
      "- Accuracy              :  46.84 %\n",
      "- Avg. inference time   :  157.6 ms\n",
      "- Throughput (tok/s)    : 2648.0\n",
      "- Avg. peak VRAM        : 2402.8 MB\n",
      "- Score Acc/VRAM        :  0.195 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_repo,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ds = load_dataset(\"brygotti/NLP4Education_english_single_mcq_4_choices\")[\"test\"]\n",
    "metrics = evaluate_mmlu(ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "display_metric(\"SFT Model\", metrics)\n",
    "all_metrics['SFT Model'] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b351c1-dc2b-453c-8646-05287cc4b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1962/1962 [07:54<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**BnB 4-bit Model Evaluation Results**\n",
      "- Accuracy              :  43.27 %\n",
      "- Avg. inference time   :  241.0 ms\n",
      "- Throughput (tok/s)    : 1731.5\n",
      "- Avg. peak VRAM        :  616.0 MB\n",
      "- Score Acc/VRAM        :  0.702 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/TheS3b/Qwen3-0.6B-bnb-4bit/commit/d33c0d03e07e8e28b6385d59666d64c53e9fb918', commit_message='Upload Qwen3ForCausalLM', commit_description='', oid='d33c0d03e07e8e28b6385d59666d64c53e9fb918', pr_url=None, repo_url=RepoUrl('https://huggingface.co/TheS3b/Qwen3-0.6B-bnb-4bit', endpoint='https://huggingface.co', repo_type='model', repo_id='TheS3b/Qwen3-0.6B-bnb-4bit'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo, trust_remote_code=True)\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_repo,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ds = load_dataset(\"brygotti/NLP4Education_english_single_mcq_4_choices\")[\"test\"]\n",
    "metrics = evaluate_mmlu(ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "display_metric(\"BnB 4-bit Model\", metrics)\n",
    "all_metrics['BnB 4-bit Model'] = metrics\n",
    "\n",
    "tokenizer.push_to_hub(\"TheS3b/Qwen3-0.6B-bnb-4bit\")\n",
    "model.push_to_hub(\"TheS3b/Qwen3-0.6B-bnb-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a45a6e-6baa-43f4-96e3-886c45a1fae6",
   "metadata": {},
   "source": [
    "Weights are stored in 4-bit but packed in 8-bit numbers. During inference, the intermediate results are stored in 16-bit float numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a384a7dd-b1bc-4c07-b928-6df70f7d8702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1962/1962 [16:28<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**BnB 8-bit Model Evaluation Results**\n",
      "- Accuracy              :  47.30 %\n",
      "- Avg. inference time   :  502.5 ms\n",
      "- Throughput (tok/s)    :  830.3\n",
      "- Avg. peak VRAM        : 1231.5 MB\n",
      "- Score Acc/VRAM        :  0.384 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/TheS3b/Qwen3-0.6B-bnb-8bit/commit/5c61abc5243fe0f7cb7c2d2ec2c0552af4bdaec6', commit_message='Upload Qwen3ForCausalLM', commit_description='', oid='5c61abc5243fe0f7cb7c2d2ec2c0552af4bdaec6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/TheS3b/Qwen3-0.6B-bnb-8bit', endpoint='https://huggingface.co', repo_type='model', repo_id='TheS3b/Qwen3-0.6B-bnb-8bit'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo, trust_remote_code=True)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_has_fp16_weight=False, # keep all weights int8 on GPU, better VRAM\n",
    "    llm_int8_enable_fp32_cpu_offload=False\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_repo,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "ds = load_dataset(\"brygotti/NLP4Education_english_single_mcq_4_choices\")[\"test\"]\n",
    "metrics = evaluate_mmlu(ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "display_metric(\"BnB 8-bit Model\", metrics)\n",
    "all_metrics['BnB 8-bit Model'] = metrics\n",
    "\n",
    "tokenizer.push_to_hub(\"TheS3b/Qwen3-0.6B-bnb-8bit\")\n",
    "model.push_to_hub(\"TheS3b/Qwen3-0.6B-bnb-8bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c93cfea5-3e4c-4949-95de-e3f1c3e1ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate save of results\n",
    "import json\n",
    "with open(\"Results/quantization_metrics.json\", \"w\") as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d29151-4393-4ff6-b290-47d8d4a53be0",
   "metadata": {},
   "source": [
    "# GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b1e3219-a1ed-4c14-bfaa-11af8235fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Results/quantization_metrics.json\", \"r\") as f:\n",
    "    all_metrics = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c98ec88-1c77-40be-ad93-ca89ce1bb222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Quantizing with 20 calibration prompts -- \n",
      "\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e068f8dbfcab4e76a40875eb5ced13ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1962/1962 [08:02<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**GPTQ Model Size 20 Evaluation Results**\n",
      "- Accuracy              :  44.60 %\n",
      "- Avg. inference time   :  244.6 ms\n",
      "- Throughput (tok/s)    : 1706.0\n",
      "- Avg. peak VRAM        :  599.3 MB\n",
      "- Score Acc/VRAM        :  0.744 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Quantizing with 200 calibration prompts -- \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c82f7a2c9649e4a9b9cf6654368df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1962/1962 [08:04<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**GPTQ Model Size 200 Evaluation Results**\n",
      "- Accuracy              :  43.83 %\n",
      "- Avg. inference time   :  245.9 ms\n",
      "- Throughput (tok/s)    : 1696.9\n",
      "- Avg. peak VRAM        :  595.0 MB\n",
      "- Score Acc/VRAM        :  0.737 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Quantizing with 2000 calibration prompts -- \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593af51b016840f48c30944f0ab2435c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1962/1962 [08:00<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**GPTQ Model Size 2000 Evaluation Results**\n",
      "- Accuracy              :  44.90 %\n",
      "- Avg. inference time   :  243.7 ms\n",
      "- Throughput (tok/s)    : 1712.2\n",
      "- Avg. peak VRAM        :  595.0 MB\n",
      "- Score Acc/VRAM        :  0.755 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "hub_prefix = \"TheS3b/Qwen3-0.6B-GPTQ-4bit-rel0.5-calib\"  # base for model names\n",
    "\n",
    "logging.disable(logging.INFO)\n",
    "os.environ[\"EXLLAMA_KERNELS_AVAILABLE\"] = \"0\"\n",
    "\n",
    "# Load tokenizer once\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo, trust_remote_code=True)\n",
    "\n",
    "# Load and filter calibration data\n",
    "calibration_data = load_dataset('TheS3b/unified-dataset-filtered-430K')\n",
    "\n",
    "def is_valid_prompt(example, min_len=64, max_len=256, thresh=0.5):\n",
    "    tokens = tokenizer(example[\"prompt\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return min_len <= tokens.shape[1] <= max_len and (example[\"relevance1\"] + example[\"relevance2\"]) * 0.5 > thresh\n",
    "\n",
    "filtered_calibration_set = calibration_data.filter(lambda ex: is_valid_prompt(ex), batched=False).shuffle(seed=42)[\"train\"]\n",
    "\n",
    "# Prompt sizes to test\n",
    "prompt_sizes = [20, 200, 2000]\n",
    "\n",
    "# Dataset to evaluate on\n",
    "eval_ds = load_dataset(\"brygotti/NLP4Education_english_single_mcq_4_choices\")[\"test\"]\n",
    "\n",
    "for size in prompt_sizes:\n",
    "    print(f\"\\n -- Quantizing with {size} calibration prompts -- \\n\")\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    calibration_prompts = filtered_calibration_set.select(range(size))[\"prompt\"]\n",
    "\n",
    "    quant_config = GPTQConfig(\n",
    "        bits=4,\n",
    "        tokenizer=model_repo,\n",
    "        dataset=calibration_prompts,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_repo, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_repo,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    metrics = evaluate_mmlu(eval_ds, model, tokenizer, device, mmlu_harness_hf)\n",
    "\n",
    "    display_metric(f\"GPTQ Model Size {size}\", metrics)\n",
    "\n",
    "    # Save under a unique key and push\n",
    "    key = f\"GPTQ 4bit calib{size}\"\n",
    "    all_metrics[key] = metrics\n",
    "\n",
    "    push_name = f\"{hub_prefix}{size}\"\n",
    "    tokenizer.push_to_hub(push_name)\n",
    "    model.push_to_hub(push_name)\n",
    "\n",
    "    with open(\"Results/quantization_metrics.json\", \"w\") as f:\n",
    "        json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddcfe1-a2dc-49ca-a923-252968e673cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp",
   "language": "python",
   "name": "mnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
